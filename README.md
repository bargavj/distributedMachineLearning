# Distributed Privacy-Preserving Empirical Risk Minimization
This work combines differential privacy and multi-party computation protocol to achieve distributed machine learning. Based on the paper "Distributed Learning without Distress: Privacy-Preserving Empirical Risk Minimization" (http://papers.nips.cc/paper/7871-distributed-learning-without-distress-privacy-preserving-empirical-risk-minimization) that has been accepted at NIPS 2018.

The code contains privacy preserving implementation of L2 Regularized Logistic Regression and Linear Regression models.

### Requirements

* Python 2.7 or above
* [Numpy](https://numpy.org)
* [Scikit Learn](https://scikit-learn.org/stable/)
* [Obliv-C](https://github.com/samee/obliv-c)
* [Absentminded Crypto Toolkit](https://bitbucket.org/jackdoerner/absentminded-crypto-kit/src/master/)
* [Cycle Utility](https://github.com/samee/cmd)

### Code Execution

Execute make files in `model_aggregate_gaussian` and `model_aggregate_laplace` directories using `make` command to obtain the respective `a.out` executable files.

Run `python model_wrapper.py`
